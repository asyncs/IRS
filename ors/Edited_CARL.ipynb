{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#PREPROCESSING"
      ],
      "metadata": {
        "id": "EIr5-TfB9nGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib"
      ],
      "metadata": {
        "id": "Qwnrw1HM94zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdflib import Graph, Literal, Namespace, URIRef\n",
        "from urllib.parse import quote\n",
        "\n",
        "\n",
        "EG = Namespace(\"http://example.com/\")\n",
        "\n",
        "def create_eg_uri(name) -> URIRef:\n",
        "    \"\"\"Convert a value to a valid example.com URI by converting it to a string,\n",
        "    URL-encoding special characters, and replacing spaces with underscores.\"\"\"\n",
        "    name_str = str(name)\n",
        "    quoted = quote(name_str.replace(\" \", \"_\"))\n",
        "    return EG[quoted]\n",
        "\n",
        "def translate_df_to_rdf(data, mapping):\n",
        "    graph = Graph()\n",
        "    num_rows = len(data.index)\n",
        "    for i in range(num_rows):\n",
        "        name = data.iloc[i][mapping[\"uri\"]]\n",
        "        row_uri = create_eg_uri(name)\n",
        "\n",
        "        outcome_value = data.iloc[i]['label_utilize']\n",
        "        outcome_predicate = create_eg_uri(\"positive\" if outcome_value == 1 else \"negative\")\n",
        "        graph.add((row_uri, outcome_predicate, Literal(outcome_value)))\n",
        "\n",
        "        for column_name, predicate_uri in mapping.items():\n",
        "            if column_name == \"uri\":\n",
        "                continue\n",
        "            predicate = create_eg_uri(predicate_uri)\n",
        "            value = data.iloc[i][column_name]\n",
        "\n",
        "            if pd.notna(value) and isinstance(value, str):\n",
        "                value = value.strip()\n",
        "            graph.add((row_uri, predicate, Literal(value)))\n",
        "    return graph\n",
        "\n",
        "\n",
        "mapping = {\n",
        "    \"uri\": \"label_utilize\",\n",
        "    \"obj0 on table_north\": \"obj0_on_table_north\",\n",
        "    \"obj0 on table_south\": \"obj0_on_table_south\",\n",
        "    \"obj0 on table_east\": \"obj0_on_table_east\",\n",
        "    \"obj0 on table_west\": \"obj0_on_table_west\",\n",
        "    \"obj1 on table_north\": \"obj1_on_table_north\",\n",
        "    \"obj1 on table_south\": \"obj1_on_table_south\",\n",
        "    \"obj1 on table_east\": \"obj1_on_table_east\",\n",
        "    \"obj1 on table_west\": \"obj1_on_table_west\",\n",
        "    \"obj2 on table_north\": \"obj2_on_table_north\",\n",
        "    \"obj2 on table_south\": \"obj2_on_table_south\",\n",
        "    \"obj2 on table_east\": \"obj2_on_table_east\",\n",
        "    \"obj2 on table_west\": \"obj2_on_table_west\",\n",
        "    \"obj3 on table_north\": \"obj3_on_table_north\",\n",
        "    \"obj3 on table_south\": \"obj3_on_table_south\",\n",
        "    \"obj3 on table_east\": \"obj3_on_table_east\",\n",
        "    \"obj3 on table_west\": \"obj3_on_table_west\",\n",
        "    \"obj4 on table_north\": \"obj4_on_table_north\",\n",
        "    \"obj4 on table_south\": \"obj4_on_table_south\",\n",
        "    \"obj4 on table_east\": \"obj4_on_table_east\",\n",
        "    \"obj4 on table_west\": \"obj4_on_table_west\",\n",
        "    \"tray on table_north\": \"tray_on_table_north\",\n",
        "    \"tray on table_south\": \"tray_on_table_south\",\n",
        "    \"tray on table_east\": \"tray_on_table_east\",\n",
        "    \"tray on table_west\": \"tray_on_table_west\",\n",
        "    \"jug on table_north\": \"jug_on_table_north\",\n",
        "    \"jug on table_south\": \"jug_on_table_south\",\n",
        "    \"jug on table_east\": \"jug_on_table_east\",\n",
        "    \"jug on table_west\": \"jug_on_table_west\",\n",
        "    \"is_helper_exist\": \"is_helper_exist\",\n",
        "    \"goal at table_north\": \"goal_at_table_north\",\n",
        "    \"goal at table_south\": \"goal_at_table_south\",\n",
        "    \"goal at table_east\": \"goal_at_table_east\",\n",
        "    \"goal at table_west\": \"goal_at_table_west\",\n",
        "    \"goal at table_watersource\": \"goal_at_table_watersource\",\n",
        "    \"goal at table_obstacle\": \"goal_at_table_obstacle\",\n",
        "    \"goal at table_handover\": \"goal_at_table_handover\",\n",
        "    \"action_pick\": \"action_pick\",\n",
        "    \"action_place\": \"action_place\",\n",
        "    \"action_carry\": \"action_carry\",\n",
        "    \"action_fill\": \"action_fill\",\n",
        "    \"action_pour\": \"action_pour\",\n",
        "    \"action_handover\": \"action_handover\",\n",
        "    \"label_utilize\": \"label_utilize\"\n",
        "}\n",
        "\n",
        "rdf_graph = translate_df_to_rdf(data_df, mapping)\n",
        "\n",
        "def extract_data_from_graph(graph):\n",
        "    data = []\n",
        "    for s, p, o in graph:\n",
        "        predicate = str(p).split('/')[-1]\n",
        "        subject = str(s).split('/')[-1]\n",
        "        object = str(o)\n",
        "        data.append((subject, predicate, object))\n",
        "    return data\n",
        "\n",
        "extracted_data = extract_data_from_graph(rdf_graph)\n",
        "\n"
      ],
      "metadata": {
        "id": "2eFLBdfX9qqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_output_file_path = '/content/CARL/datasets/tic/facts.txt'\n",
        "with open(clean_output_file_path, 'w') as file:\n",
        "    for entry in extracted_data:\n",
        "        formatted_entry = \" \".join(entry).replace(\"(\", \"\").replace(\")\", \"\").replace(\"'\", \"\").replace(\",\", \"\")\n",
        "        parts = formatted_entry.split()\n",
        "        formatted_output = f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\n\"\n",
        "        file.write(formatted_output)\n",
        "\n",
        "clean_output_file_path\n"
      ],
      "metadata": {
        "id": "e1SdsVYi-BC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "entities_file_path = '/content/CARL/datasets/tic/entities.txt'\n",
        "relations_file_path = '/content/CARL/datasets/tic/relations.txt'\n",
        "entities = set()\n",
        "relations = set()\n",
        "\n",
        "for s, p, o in extracted_data:\n",
        "    entities.add(s)\n",
        "    entities.add(o)\n",
        "    relations.add(p)\n",
        "\n",
        "def clean_uri(uri):\n",
        "    return str(uri).split('/')[-1]\n",
        "\n",
        "with open(entities_file_path, 'w') as file:\n",
        "    for entity in sorted(entities, key=lambda x: str(x)):\n",
        "        file.write(f\"{clean_uri(entity)}\\n\")\n",
        "\n",
        "with open(relations_file_path, 'w') as file:\n",
        "    for relation in sorted(relations, key=lambda x: str(x)):\n",
        "        file.write(f\"{clean_uri(relation)}\\n\")\n",
        "\n",
        "entities_file_path, relations_file_path\n"
      ],
      "metadata": {
        "id": "TTmDL7h1-IdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "file_path = '/content/facts.txt'\n",
        "\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "\n",
        "train_split = 0.5\n",
        "valid_split = 0.25\n",
        "test_split = 0.25\n",
        "\n",
        "random.shuffle(lines)\n",
        "train_end = int(len(lines) * train_split)\n",
        "train_data = lines[:train_end]\n",
        "\n",
        "\n",
        "random.shuffle(lines)\n",
        "valid_end = int(len(lines) * valid_split)\n",
        "valid_data = lines[:valid_end]\n",
        "\n",
        "\n",
        "random.shuffle(lines)\n",
        "test_data = lines[:int(len(lines) * test_split)]\n",
        "\n",
        "\n",
        "with open('/content/CARL/datasets/tic/train.txt', 'w') as f:\n",
        "    f.writelines(train_data)\n",
        "\n",
        "with open('/content/CARL/datasets/tic/valid.txt', 'w') as f:\n",
        "    f.writelines(valid_data)\n",
        "\n",
        "with open('/content/CARL/datasets/tic/test.txt', 'w') as f:\n",
        "    f.writelines(test_data)\n",
        "\n",
        "print(\"Data split and saved to train.txt, valid.txt, and test.txt\")\n"
      ],
      "metadata": {
        "id": "ikrv7Gqj-QnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_path = '/content/facts.txt'\n",
        "inverted_path = '/content/CARL/datasets/tic/facts.txt.inv'\n",
        "\n",
        "\n",
        "with open(original_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "inverted_lines = []\n",
        "for line in lines:\n",
        "    parts = line.strip().split()\n",
        "    if len(parts) == 3:\n",
        "        inverted_line = f\"{parts[2]}\\tinv_{parts[1]}\\t{parts[0]}\\n\"\n",
        "        inverted_lines.append(inverted_line)\n",
        "    else:\n",
        "        inverted_lines.append(line)\n",
        "\n",
        "with open(inverted_path, 'w') as file:\n",
        "    file.writelines(inverted_lines)\n",
        "\n",
        "print(\"Inverted file saved as facts.txt.inv\")\n"
      ],
      "metadata": {
        "id": "gzevE1Va_CJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CARL MODEL"
      ],
      "metadata": {
        "id": "0BUMNvld9CFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/burning5112/CARL.git\n",
        "%cd /content/CARL"
      ],
      "metadata": {
        "id": "reVtNblE_33H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZCSd7Be86cR"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from random import sample\n",
        "import random\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import time\n",
        "\n",
        "\n",
        "def construct_fact_dict(fact_rdf):\n",
        "    fact_dict = {}\n",
        "    for rdf in fact_rdf:\n",
        "        fact = parse_rdf(rdf)\n",
        "        h, r, t = fact\n",
        "        if r not in fact_dict:\n",
        "            fact_dict[r] = []\n",
        "        fact_dict[r].append(rdf)\n",
        "    return fact_dict\n",
        "\n",
        "\n",
        "def parse_rdf(rdf):\n",
        "    print(rdf)\n",
        "    rdf_tail, rdf_rel, rdf_head = rdf\n",
        "    return rdf_head, rdf_rel, rdf_tail\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.rel2idx_ = {}\n",
        "        self.idx2rel_ = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_relation(self, rel):\n",
        "        if rel not in self.rel2idx_.keys():\n",
        "            self.rel2idx_[rel] = self.idx\n",
        "            self.idx2rel_[self.idx] = rel\n",
        "            self.idx += 1\n",
        "\n",
        "    @property\n",
        "    def rel2idx(self):\n",
        "        return self.rel2idx_\n",
        "\n",
        "    @property\n",
        "    def idx2rel(self):\n",
        "        return self.idx2rel_\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2rel_)\n",
        "\n",
        "\n",
        "def load_entities(path):\n",
        "    idx2ent, ent2idx = {}, {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for idx, line in enumerate(lines):\n",
        "            \"\"\"\n",
        "            这一行使用for循环遍历lines列表中的每一项，并使用enumerate函数给每一项添加一个索引。索引赋值给变量idx，字符串赋值给变量line。\n",
        "            \"\"\"\n",
        "            e = line.strip()\n",
        "            ent2idx[e] = idx\n",
        "            idx2ent[idx] = e\n",
        "    return idx2ent, ent2idx\n",
        "\n",
        "\n",
        "class Dataset(object):\n",
        "    def __init__(self, data_root, sparsity=1, inv=False):\n",
        "\n",
        "        entity_path = data_root + 'entities.txt'\n",
        "        self.idx2ent_, self.ent2idx_ = load_entities(entity_path)\n",
        "\n",
        "        relation_path = data_root + 'relations.txt'\n",
        "        self.rdict = Dictionary()\n",
        "        self.load_relation_dict(relation_path)\n",
        "\n",
        "        self.head_rdict = Dictionary()\n",
        "        self.head_rdict = copy.deepcopy(self.rdict)\n",
        "\n",
        "        fact_path = data_root + 'facts.txt'\n",
        "        train_path = data_root + 'train.txt'\n",
        "        valid_path = data_root + 'valid.txt'\n",
        "        test_path = data_root + 'test.txt'\n",
        "\n",
        "        if inv:\n",
        "            fact_path += '.inv'\n",
        "\n",
        "        self.rdf_data_ = self.load_data_(fact_path, train_path, valid_path, test_path, sparsity)\n",
        "        self.fact_rdf_, self.train_rdf_, self.valid_rdf_, self.test_rdf_ = self.rdf_data_\n",
        "\n",
        "        if inv:\n",
        "\n",
        "            rel_list = list(self.rdict.rel2idx_.keys())\n",
        "            for rel in rel_list:\n",
        "                inv_rel = \"inv_\" + rel\n",
        "                self.rdict.add_relation(inv_rel)\n",
        "                self.head_rdict.add_relation(inv_rel)\n",
        "\n",
        "        self.head_rdict.add_relation(\"None\")\n",
        "\n",
        "    def load_rdfs(self, path):\n",
        "        rdf_list = []\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                tuples = line.strip().split('\\t')\n",
        "                rdf_list.append(tuples)\n",
        "        return rdf_list\n",
        "\n",
        "    def load_data_(self, fact_path, train_path, valid_path, test_path, sparsity):\n",
        "        fact = self.load_rdfs(fact_path)\n",
        "        fact = sample(fact, int(len(fact) * sparsity))\n",
        "\n",
        "        train = self.load_rdfs(train_path)\n",
        "        valid = self.load_rdfs(valid_path)\n",
        "        test = self.load_rdfs(test_path)\n",
        "        return fact, train, valid, test\n",
        "\n",
        "    def load_relation_dict(self, relation_path):\n",
        "        \"\"\"\n",
        "        Read relation.txt to relation dictionary\n",
        "        \"\"\"\n",
        "        with open(relation_path, encoding='utf-8') as f:\n",
        "            rel_list = f.readlines()\n",
        "            for r in rel_list:\n",
        "                relation = r.strip()\n",
        "                self.rdict.add_relation(relation)\n",
        "\n",
        "    def get_relation_dict(self):\n",
        "        return self.rdict\n",
        "\n",
        "    def get_head_relation_dict(self):\n",
        "        return self.head_rdict\n",
        "\n",
        "    @property\n",
        "    def idx2ent(self):\n",
        "        return self.idx2ent_\n",
        "\n",
        "    @property\n",
        "    def ent2idx(self):\n",
        "        return self.ent2idx_\n",
        "\n",
        "    @property\n",
        "    def fact_rdf(self):\n",
        "        return self.fact_rdf_\n",
        "\n",
        "    @property\n",
        "    def train_rdf(self):\n",
        "        return self.train_rdf_\n",
        "\n",
        "    @property\n",
        "    def valid_rdf(self):\n",
        "        return self.valid_rdf_\n",
        "\n",
        "    @property\n",
        "    def test_rdf(self):\n",
        "        return self.test_rdf_\n",
        "\n",
        "\n",
        "def sample_anchor_rdf(rdf_data, num):\n",
        "    if num < len(rdf_data):\n",
        "        return sample(rdf_data, num)\n",
        "\n",
        "\n",
        "    else:\n",
        "        return rdf_data\n",
        "\n",
        "\n",
        "def construct_descendant(rdf_data):\n",
        "    entity2desced = {}\n",
        "    for rdf_ in rdf_data:\n",
        "        print(rdf_)\n",
        "        h_, r_, t_ = parse_rdf(rdf_)\n",
        "        if h_ not in entity2desced.keys():\n",
        "            entity2desced[h_] = [(r_, t_)]\n",
        "        else:\n",
        "            entity2desced[h_].append((r_, t_))\n",
        "    return entity2desced\n",
        "\n",
        "\n",
        "def connected(entity2desced, head, tail):\n",
        "    if head in entity2desced:\n",
        "        decedents = entity2desced[head]\n",
        "        for d in decedents:\n",
        "            d_relation_, d_tail_ = d\n",
        "            if d_tail_ == tail:\n",
        "                return d_relation_\n",
        "        return False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def construct_rule_seq(rdf_data, anchor_rdf, entity2desced, sample_max_path_len=2, PRINT=False):\n",
        "    len2seq = {}\n",
        "    anchor_h, anchor_r, anchor_t = parse_rdf(anchor_rdf)\n",
        "\n",
        "    stack = [(anchor_h, anchor_r, anchor_t)]\n",
        "\n",
        "    stack_print = ['{}-{}-{}'.format(anchor_h, anchor_r, anchor_t)]\n",
        "\n",
        "    pre_path = anchor_h\n",
        "    rule_seq, expended_node = [], []\n",
        "    record = []\n",
        "    while len(stack) > 0:\n",
        "        cur_h, cur_r, cur_t = stack.pop(-1)\n",
        "        cur_print = stack_print.pop(-1)\n",
        "        deced_list = []\n",
        "\n",
        "        if cur_t in entity2desced:\n",
        "            deced_list = entity2desced[cur_t]\n",
        "\n",
        "        if len(cur_r.split('|')) < sample_max_path_len and len(deced_list) > 0 and cur_t not in expended_node:\n",
        "            for r_, t_ in deced_list:\n",
        "                if t_ != cur_h and t_ != anchor_h:\n",
        "                    stack.append((cur_t, cur_r + '|' + r_, t_))\n",
        "                    stack_print.append(cur_print + '-{}-{}'.format(r_, t_))\n",
        "        expended_node.append(cur_t)\n",
        "\n",
        "        rule_head_rel = connected(entity2desced, anchor_h, cur_t)\n",
        "        if rule_head_rel and cur_t != anchor_t:\n",
        "            rule = cur_r + '-' + rule_head_rel\n",
        "            rule_seq.append(rule)\n",
        "            if (cur_h, r_, t_) not in record:\n",
        "                record.append((cur_h, r_, t_))\n",
        "            if PRINT:\n",
        "                print('rule body:\\n{}'.format(cur_print))\n",
        "                print('rule head:\\n{}-{}-{}'.format(anchor_h, rule_head_rel, cur_t))\n",
        "                print('rule:\\n{}\\n'.format(rule))\n",
        "        elif rule_head_rel == False and random.random() > 0.9:\n",
        "            rule = cur_r + '-' + \"None\"\n",
        "            rule_seq.append(rule)\n",
        "            if (cur_h, r_, t_) not in record:\n",
        "                record.append((cur_h, r_, t_))\n",
        "            if PRINT:\n",
        "                print('rule body:\\n{}'.format(cur_print))\n",
        "                print('rule head:\\n{}-{}-{}'.format(anchor_h, rule_head_rel, cur_t))\n",
        "                print('rule:\\n{}\\n'.format(rule))\n",
        "    return rule_seq, record\n",
        "\n",
        "\n",
        "def body2idx(body_list, head_rdict):\n",
        "    res = []\n",
        "    for body in body_list:\n",
        "        body_path = body.split('|')\n",
        "\n",
        "        indexs = []\n",
        "        for rel in body_path:\n",
        "            indexs.append(head_rdict.rel2idx[rel])\n",
        "        res.append(indexs)\n",
        "    return res\n",
        "\n",
        "\n",
        "def inv_rel_idx(head_rdict):\n",
        "    inv_rel_idx = []\n",
        "    for i_ in range(len(head_rdict.idx2rel)):\n",
        "        r_ = head_rdict.idx2rel[i_]\n",
        "        if \"inv_\" in r_:\n",
        "            inv_rel_idx.append(i_)\n",
        "    return inv_rel_idx\n",
        "\n",
        "\n",
        "def idx2body(index, head_rdict):\n",
        "    body = \"|\".join([head_rdict.idx2rel[idx] for idx in index])\n",
        "    return body\n",
        "\n",
        "\n",
        "def rule2idx(rule, head_rdict):\n",
        "    body, head = rule.split('-')\n",
        "    body_path = body.split('|')\n",
        "\n",
        "    indexs = []\n",
        "    for rel in body_path + [-1, head]:\n",
        "        indexs.append(head_rdict.rel2idx[rel] if rel != -1 else -1)\n",
        "    return indexs\n",
        "\n",
        "\n",
        "def idx2rule(index, head_rdict):\n",
        "    body_idx = index[0:-2]\n",
        "    body = \"|\".join([head_rdict.idx2rel[b] for b in body_idx])\n",
        "    rule = body + \"-\" + head_rdict.idx2rel[index[-1]]\n",
        "    return rule\n",
        "\n",
        "\n",
        "def enumerate_body(relation_num, body_len, rdict):\n",
        "    import itertools\n",
        "\n",
        "    all_body_idx = list(list(x) for x in itertools.product(range(relation_num), repeat=body_len))\n",
        "    \"\"\"\n",
        "    product(A, B, C) = ((a, b, c) for a in A for b in B for c in C)\n",
        "    product(A, repeat=2) = product(A, A) = ((a1, a2) for a1 in A for a2 in A)\n",
        "    \"\"\"\n",
        "    idx2rel = rdict.idx2rel\n",
        "    all_body = []\n",
        "    for b_idx_ in all_body_idx:\n",
        "        b_ = [idx2rel[x] for x in b_idx_]\n",
        "        all_body.append(b_)\n",
        "    return all_body_idx, all_body\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_msg(msg):\n",
        "\n",
        "    msg = \"## {} ##\".format(msg)\n",
        "    length = len(msg)\n",
        "    msg = \"\\n{}\\n\".format(msg)\n",
        "    print(length*\"#\" + msg + length * \"#\")\n",
        "import torch\n",
        "def cross_entropy(y,y_pre,weight):\n",
        "\n",
        "    res = weight * (y*torch.log(y_pre))\n",
        "    loss=-torch.sum(res)\n",
        "    return loss/y_pre.shape[0]\n",
        "\n",
        "    model_file = 'epochs:{}_alpha:{}_anchor:{}_sample_max_path_len:{}_embedding_size{}_learning_rate{}_batch_size{}_get_top_k{}'.format(\n",
        "        args.epochs,\n",
        "        args.alpha,\n",
        "        args.anchor,\n",
        "        args.sample_max_path_len,\n",
        "        args.embedding_size,\n",
        "        args.learning_rate,\n",
        "        args.batch_size,\n",
        "        args.get_top_k\n",
        "    )\n",
        "    os.makedirs(\"../rules/{}/{}/{}\".format(args.model,args.datasets,model_file), exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Ai3YWOB59HrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.distributions import Categorical\n",
        "import math\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=3):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class ProbAttention(nn.Module):\n",
        "    def __init__(self, mask_flag=False, factor=5, scale=None, attention_dropout=0.1, output_attention=True):\n",
        "        super(ProbAttention, self).__init__()\n",
        "        self.factor = factor\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def _prob_QK(self, Q, K, sample_k, n_top):\n",
        "        B, H, L_K, E = K.shape\n",
        "        _, _, L_Q, _ = Q.shape\n",
        "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
        "        index_sample = torch.randint(L_K, (L_Q, sample_k))\n",
        "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
        "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n",
        "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
        "        M_top = M.topk(n_top, sorted=False)[1]\n",
        "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
        "                   torch.arange(H)[None, :, None],\n",
        "                   M_top, :]\n",
        "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))\n",
        "        return Q_K, M_top\n",
        "\n",
        "    def _get_initial_context(self, V, L_Q):\n",
        "        B, H, L_V, D = V.shape\n",
        "        if not self.mask_flag:\n",
        "            V_sum = V.mean(dim=-2)\n",
        "            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
        "        else:\n",
        "            assert (L_Q == L_V)\n",
        "            contex = V.cumsum(dim=-2)\n",
        "        return contex\n",
        "\n",
        "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
        "        B, H, L_V, D = V.shape\n",
        "        if self.mask_flag:\n",
        "            attn_mask = ProbMask(B, H, L_Q, index, scores,\n",
        "                                 device=V.device)\n",
        "            scores.masked_fill_(attn_mask.mask,\n",
        "                                -np.inf)\n",
        "        attn = torch.softmax(scores,\n",
        "                             dim=-1)\n",
        "        context_in[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = torch.matmul(attn,\n",
        "                                                                                                            V).type_as(\n",
        "            context_in)\n",
        "        if self.output_attention:\n",
        "            attns = (torch.ones([B,\n",
        "                                 H,\n",
        "                                 L_V,\n",
        "                                 L_V]) / L_V).type_as(attn).to(attn.device)\n",
        "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
        "            return (context_in, attns)\n",
        "        else:\n",
        "            return (context_in, None)\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask=None):\n",
        "        B, L_Q, H, D = queries.shape\n",
        "        _, L_K, _, _ = keys.shape\n",
        "        queries = queries.transpose(2, 1)\n",
        "        keys = keys.transpose(2, 1)\n",
        "        values = values.transpose(2, 1)\n",
        "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()\n",
        "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()\n",
        "        U_part = U_part if U_part < L_K else L_K\n",
        "        u = u if u < L_Q else L_Q\n",
        "        scores_top, index = self._prob_QK(queries,\n",
        "                                          keys,\n",
        "                                          sample_k=U_part,\n",
        "                                          n_top=u)\n",
        "        scale = self.scale or 1. / math.sqrt(D)\n",
        "        if scale is not None:\n",
        "            scores_top = scores_top * scale\n",
        "        context = self._get_initial_context(values, L_Q)\n",
        "        context, attn = self._update_context(context,\n",
        "                                             values,\n",
        "                                             scores_top,\n",
        "                                             index,\n",
        "                                             L_Q,\n",
        "                                             attn_mask)\n",
        "        return context.transpose(2, 1).contiguous(), attn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, relation_num, emb_size, device):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.emb = nn.Embedding(relation_num + 1, emb_size, padding_idx=relation_num)\n",
        "        self.hidden_size = emb_size\n",
        "        self.relation_num = relation_num\n",
        "        self.emb_size = emb_size\n",
        "        self.device = device\n",
        "        self.dropout_rate = 0.1\n",
        "        self.num_heads = 8\n",
        "        self.num_layers = 1\n",
        "        self.slider_encoder = nn.LSTM(input_size=emb_size,\n",
        "                                      hidden_size=self.hidden_size,\n",
        "                                      num_layers=self.num_layers,\n",
        "                                      batch_first=True,\n",
        "                                      dropout=0\n",
        "                                      )\n",
        "        self.fc = nn.Linear(self.hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.w_1 = nn.Linear(emb_size, emb_size)\n",
        "        self.w_2 = nn.Linear(emb_size, emb_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.weight_w_1 = nn.Linear(self.num_heads * (relation_num + 1), self.num_heads * (relation_num + 1))\n",
        "        self.weight_w_2 = nn.Linear(self.num_heads * (relation_num + 1), relation_num + 1)\n",
        "        self.layernorm1 = nn.LayerNorm(relation_num + 1)\n",
        "        self.weight_a_1 = nn.Linear(self.num_heads * (relation_num + 1), self.num_heads * (relation_num + 1))\n",
        "        self.weight_a_2 = nn.Linear(self.num_heads * (relation_num + 1), relation_num + 1)\n",
        "        self.layernorm2 = nn.LayerNorm(relation_num + 1)\n",
        "        self.slider1 = nn.Linear(2 * emb_size, 2 * emb_size)\n",
        "        self.slider2 = nn.Linear(2 * emb_size, emb_size)\n",
        "        self.slider_3_1 = nn.Linear(2 * emb_size, 2 * emb_size)\n",
        "        self.slider_3_2 = nn.Linear(2 * emb_size, emb_size)\n",
        "        assert self.emb_size % self.num_heads == 0\n",
        "        self.head_emb_size = self.emb_size // self.num_heads\n",
        "        self.fc_q = nn.Linear(emb_size, emb_size)\n",
        "        self.fc_k = nn.Linear(emb_size, emb_size)\n",
        "        self.fc_v = nn.Linear(emb_size, emb_size)\n",
        "        self.out = nn.Linear(emb_size, emb_size)\n",
        "        self.self_fc_q = nn.Linear(emb_size, emb_size)\n",
        "        self.self_fc_k = nn.Linear(emb_size, emb_size)\n",
        "        self.self_fc_v = nn.Linear(emb_size, emb_size)\n",
        "        self.self_out = nn.Linear(emb_size, emb_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "        self.layernorm = nn.LayerNorm(emb_size)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=emb_size, max_len=3)\n",
        "        self.probAttention = ProbAttention()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.emb(inputs)\n",
        "        a = self.position_embedding(inputs)\n",
        "        inputs = inputs + a\n",
        "        batch_size, seq_len, emb_size = inputs.shape\n",
        "        idx_ = torch.LongTensor(range(self.relation_num)).repeat(batch_size, 1).to(self.device)\n",
        "        relation_emb_ori = self.emb(idx_)\n",
        "        relation_emb = relation_emb_ori.reshape(batch_size, self.relation_num, self.num_heads, self.head_emb_size)\n",
        "        relation_emb, relation_emb_weigth = self.probAttention(queries=relation_emb, keys=relation_emb,\n",
        "                                                               values=relation_emb)\n",
        "        relation_emb = relation_emb.reshape(batch_size, self.relation_num, -1)\n",
        "        relation_emb = self.layernorm(relation_emb_ori + 0.1 * relation_emb)\n",
        "        L = [inputs]\n",
        "        loss_list = []\n",
        "        idx = 0\n",
        "        while idx < seq_len - 1:\n",
        "            output, loss = self.reduce_rel_pairs(L[-1], relation_emb)\n",
        "            L.append(output)\n",
        "            loss_list.append(loss)\n",
        "            idx += 1\n",
        "        selected_rel_pair_after_attention_value, attn_weights, scores = self.multiHeadAttention(L[-1], relation_emb)\n",
        "        loss = Categorical(probs=attn_weights).entropy()\n",
        "        loss_list.append(loss)\n",
        "        loss_tensor = torch.cat(loss_list, dim=-1)\n",
        "        return self.predict_head(scores), loss_tensor, relation_emb_weigth\n",
        "\n",
        "    def reduce_rel_pairs(self, inputs, relation_emb_with_attention):\n",
        "\n",
        "        batch_size, seq_len, emb_size = inputs.shape\n",
        "        if seq_len > 2:\n",
        "            rel_pairs = []\n",
        "            idx = 0\n",
        "            while idx < seq_len - 1:\n",
        "                rel_pairs_emb = inputs[:, idx:idx + 2, :]\n",
        "                rel_pairs_emb = rel_pairs_emb.reshape(batch_size, -1)\n",
        "                rel_pairs_emb = self.dropout(self.relu(self.slider1(rel_pairs_emb)))\n",
        "                rel_pairs_emb = self.dropout(self.slider2(rel_pairs_emb))\n",
        "                h = self.layernorm(rel_pairs_emb)\n",
        "                rel_pairs.append(h)\n",
        "                idx += 1\n",
        "            rel_pairs = torch.stack(rel_pairs, dim=1)\n",
        "            choice_rel_pairs = self.dropout(self.fc(rel_pairs)).squeeze(-1)\n",
        "            choice_rel_pairs = self.sigmoid(choice_rel_pairs)\n",
        "            selected_rel_pair_idx = torch.argmax(choice_rel_pairs,\n",
        "                                                 dim=-1)\n",
        "            full_batch = torch.arange(batch_size).to(self.device)\n",
        "            selected_rel_pair = rel_pairs[full_batch, selected_rel_pair_idx, :]\n",
        "            selected_rel_pair = selected_rel_pair.unsqueeze(1)\n",
        "            selected_rel_pair_after_attention_value, attn_weights, scores = self.multiHeadAttention(selected_rel_pair,\n",
        "                                                                                                    relation_emb_with_attention)\n",
        "            selected_rel_pair_after_attention_value = self.feedForward(selected_rel_pair_after_attention_value)\n",
        "            loss = Categorical(probs=attn_weights).entropy()\n",
        "            selected_rel_pair_after_attention_value = selected_rel_pair_after_attention_value.squeeze(1)\n",
        "            output = inputs.detach().clone()\n",
        "            zero = torch.zeros(emb_size).to(self.device)\n",
        "            output[full_batch, selected_rel_pair_idx, :] = selected_rel_pair_after_attention_value\n",
        "            output[full_batch, selected_rel_pair_idx + 1, :] = zero\n",
        "            output = output[~torch.all(output == 0, dim=-1)]\n",
        "            output = output.reshape(batch_size, -1, emb_size)\n",
        "\n",
        "        else:\n",
        "\n",
        "            inter = inputs.reshape(batch_size, -1)\n",
        "            inter = self.dropout(self.relu(self.slider1(inter)))\n",
        "            output = self.dropout(self.slider2(inter))\n",
        "            output = self.layernorm(output).unsqueeze(1)\n",
        "            loss = torch.zeros((batch_size, 1)).to(self.device)\n",
        "        return output, loss\n",
        "\n",
        "    def multiHeadAttention(self, inputs, relation_emb, mask=None):\n",
        "        batch_size, seq_len, emb_size = inputs.shape\n",
        "        query = self.dropout(self.fc_q(inputs)).view(batch_size, seq_len, self.num_heads, self.head_emb_size).transpose(\n",
        "            1, 2)\n",
        "        key = self.dropout(self.fc_k(torch.cat((relation_emb, inputs), dim=1))).view(batch_size, -1, self.num_heads,\n",
        "                                                                                     self.head_emb_size).transpose(1, 2)\n",
        "        value = self.dropout(self.fc_v(torch.cat((relation_emb, inputs), dim=1))).view(batch_size, -1, self.num_heads,\n",
        "                                                                                       self.head_emb_size).transpose(1,\n",
        "                                                                                                                     2)\n",
        "\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_emb_size)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, value)\n",
        "\n",
        "        output = self.out(attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
        "\n",
        "        scores = torch.mean(scores, dim=1)\n",
        "        attn_weights = torch.mean(attn_weights, dim=1)\n",
        "\n",
        "        return self.layernorm(output + inputs), attn_weights, scores\n",
        "\n",
        "    def feedForward(self, inputs):\n",
        "        inter = self.dropout(self.relu(self.w_1(inputs)))\n",
        "        output = self.dropout(self.w_2(inter))\n",
        "        return self.layernorm(output + inputs)\n",
        "\n",
        "    def transformer_attention(self, inputs, relation_emb):\n",
        "        batch_size, seq_len, emb_size = inputs.shape\n",
        "        query = self.dropout(self.fc_q(inputs))\n",
        "        key = self.dropout(self.fc_k(torch.cat((relation_emb, inputs), dim=1)))\n",
        "        value = self.dropout(self.fc_v(torch.cat((relation_emb, inputs), dim=1)))\n",
        "        scores_ori = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "                     / math.sqrt(self.emb_size)\n",
        "        mask1 = torch.zeros((batch_size, seq_len, self.relation_num), dtype=torch.bool).to(self.device)\n",
        "        I = torch.eye(seq_len).to(self.device)\n",
        "        I = I.reshape((1, seq_len, seq_len))\n",
        "        I = I.repeat(batch_size, 1, 1)\n",
        "        mask2 = ~I.to(torch.bool)\n",
        "        mask = torch.cat((mask1, mask2), dim=-1)\n",
        "        scores = scores_ori\n",
        "        scores[mask] = float('-inf')\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        output = attn_weights @ value\n",
        "        return self.layernorm(output), attn_weights, scores_ori\n",
        "\n",
        "    def self_attention(self, relation_emb, mask=None):\n",
        "        batch_size, seq_len, emb_size = relation_emb.shape\n",
        "\n",
        "        query = self.dropout(self.self_fc_q(relation_emb)).view(batch_size, seq_len, self.num_heads,\n",
        "                                                                self.head_emb_size).transpose(1, 2)\n",
        "        key = self.dropout(self.self_fc_k(relation_emb)).view(batch_size, seq_len, self.num_heads,\n",
        "                                                              self.head_emb_size).transpose(1, 2)\n",
        "        value = self.dropout(self.self_fc_v(relation_emb)).view(batch_size, seq_len, self.num_heads,\n",
        "                                                                self.head_emb_size).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_emb_size)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, value)\n",
        "\n",
        "        output = self.self_out(attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
        "\n",
        "        scores = torch.mean(scores, dim=1)\n",
        "        attn_weights = torch.mean(attn_weights, dim=1)\n",
        "\n",
        "        return self.layernorm(output + relation_emb), attn_weights, scores\n",
        "\n",
        "    def predict_head(self, prob):\n",
        "        return prob.squeeze(1)\n",
        "\n",
        "    def get_relation_emb(self, rel):\n",
        "        return self.emb(rel)\n",
        "\n",
        "    def weightedAverage(self, scores, inputs):\n",
        "        batch_size, seq_len, _ = scores.shape\n",
        "        mask1 = torch.zeros((batch_size, seq_len, self.relation_num), dtype=torch.bool).to(self.device)\n",
        "        I = torch.eye(seq_len).to(self.device)\n",
        "        I = I.reshape((1, seq_len, seq_len))\n",
        "        I = I.repeat(batch_size, 1, 1)\n",
        "        mask2 = ~I.to(torch.bool)\n",
        "        mask = torch.cat((mask1, mask2), dim=-1)\n",
        "        scores[mask] = float('-inf')\n",
        "        prob = self.dropout(torch.softmax(scores, dim=-1))\n",
        "        idx_ = torch.LongTensor(range(self.relation_num)).repeat(batch_size, 1).to(self.device)\n",
        "        relation_emb = self.emb(idx_)\n",
        "        all_emb = torch.cat((relation_emb, inputs), dim=1)\n",
        "        out = prob @ all_emb\n",
        "        return self.layernorm(out)\n"
      ],
      "metadata": {
        "id": "9BqlflLc9KRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from audioop import reverse\n",
        "from wsgiref import headers\n",
        "from xml.dom.minidom import Element\n",
        "\n",
        "import copy\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.multiprocessing as mp\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from collections import defaultdict\n",
        "import argparse\n",
        "import gc\n",
        "import os\n",
        "import sys\n",
        "import io\n",
        "\n",
        "\n",
        "\n",
        "head2Mean_rank_mrr = defaultdict(list)\n",
        "head2Mean_rank_hit_1 = defaultdict(list)\n",
        "head2Mean_rank_hit_10 = defaultdict(list)\n",
        "head2Top_rank_mrr = defaultdict(list)\n",
        "head2Top_rank_hit_1 = defaultdict(list)\n",
        "head2Top_rank_hit_10 = defaultdict(list)\n",
        "Mean_rank = defaultdict(list)\n",
        "Top_rank = defaultdict(list)\n",
        "\n",
        "\n",
        "class RuleDataset(Dataset):\n",
        "    def __init__(self, r2mat, rules, e_num, idx2rel, args):\n",
        "        self.e_num = e_num\n",
        "        self.r2mat = r2mat\n",
        "        self.rules = rules\n",
        "        self.idx2rel = idx2rel\n",
        "        self.len = len(self.rules)\n",
        "        self.args = args\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rel = self.idx2rel[idx]\n",
        "        _rules = self.rules[rel]\n",
        "\n",
        "        path_count = sparse.dok_matrix((self.e_num, self.e_num))\n",
        "        for rule in _rules:\n",
        "            head, body, conf_1, conf_2 = rule\n",
        "            if conf_1 >= self.args.threshold:\n",
        "                body_adj = sparse.eye(self.e_num)\n",
        "                for b_rel in body:\n",
        "                    body_adj = body_adj * self.r2mat[b_rel]\n",
        "\n",
        "                body_adj = body_adj * conf_1\n",
        "                path_count += body_adj\n",
        "                del body_adj\n",
        "\n",
        "        return rel, path_count\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(data):\n",
        "        head = [_[0] for _ in data]\n",
        "        path_count1 = [_[1] for _ in data]\n",
        "        return head, path_count1\n",
        "\n",
        "\n",
        "def sum_from_n_to_len(n_non_zero, filtered_pred):\n",
        "    total = 0\n",
        "\n",
        "    for i in range(n_non_zero + 1, len(filtered_pred) + 1):\n",
        "        total += i\n",
        "\n",
        "    return total\n",
        "\n",
        "\n",
        "def sortSparseMatrix(m, r, rev=True, only_indices=False):\n",
        "    d = m.getrow(r)\n",
        "    s = zip(d.indices, d.data)\n",
        "    sorted_s = sorted(s, key=lambda v: v[1], reverse=rev)\n",
        "    if only_indices:\n",
        "        res = [element[0] for element in sorted_s]\n",
        "    else:\n",
        "        res = sorted_s\n",
        "    return res\n",
        "\n",
        "\n",
        "def remove_var(r):\n",
        "    r = re.sub(r\"\\(\\D?, \\D?\\)\", \"\", r)\n",
        "    return r\n",
        "\n",
        "\n",
        "def parse_rule(r):\n",
        "    r = remove_var(r)\n",
        "    head, body = r.split(\" <-- \")\n",
        "    body = body.split(\", \")\n",
        "    return head, body\n",
        "\n",
        "\n",
        "def load_rules(rule_path, all_rules, all_heads):\n",
        "    with open(rule_path, 'r') as f:\n",
        "        rules = f.readlines()\n",
        "        for i_, rule in enumerate(rules):\n",
        "            conf, r = rule.strip('\\n').split('\\t')\n",
        "            conf_1, conf_2 = float(conf[0:5]), float(conf[-6:-1])\n",
        "            head, body = parse_rule(r)\n",
        "\n",
        "            if head not in all_rules:\n",
        "                all_rules[head] = []\n",
        "            all_rules[head].append((head, body, conf_1, conf_2))\n",
        "\n",
        "            if head not in all_heads:\n",
        "                all_heads.append(head)\n",
        "\n",
        "\n",
        "def construct_rmat(idx2rel, idx2ent, ent2idx, fact_rdf):\n",
        "    e_num = len(idx2ent)\n",
        "    r2mat = {}\n",
        "    for idx, rel in idx2rel.items():\n",
        "        mat = sparse.dok_matrix((e_num, e_num))\n",
        "        r2mat[rel] = mat\n",
        "\n",
        "    for rdf in fact_rdf:\n",
        "        fact = parse_rdf(rdf)\n",
        "        h, r, t = fact\n",
        "        h_idx, t_idx = ent2idx[h], ent2idx[t]\n",
        "        r2mat[r][h_idx, t_idx] = 1\n",
        "    return r2mat\n",
        "\n",
        "\n",
        "def get_gt(dataset):\n",
        "    idx2ent, ent2idx = dataset.idx2ent, dataset.ent2idx\n",
        "    fact_rdf, train_rdf, valid_rdf, test_rdf = dataset.fact_rdf, dataset.train_rdf, dataset.valid_rdf, dataset.test_rdf\n",
        "    gt = defaultdict(list)\n",
        "    all_rdf = fact_rdf + train_rdf + valid_rdf + test_rdf\n",
        "    for rdf in all_rdf:\n",
        "        h, r, t = parse_rdf(rdf)\n",
        "        gt[(h, r)].append(ent2idx[t])\n",
        "    return gt\n",
        "\n",
        "\n",
        "def kg_completion(rules, dataset, args):\n",
        "    fact_rdf, train_rdf, valid_rdf, test_rdf = dataset.fact_rdf, dataset.train_rdf, dataset.valid_rdf, dataset.test_rdf\n",
        "\n",
        "    gt = get_gt(dataset)\n",
        "\n",
        "    rdict = dataset.get_relation_dict()\n",
        "    head_rdict = dataset.get_head_relation_dict()\n",
        "    rel2idx, idx2rel = rdict.rel2idx, rdict.idx2rel\n",
        "\n",
        "    idx2ent, ent2idx = dataset.idx2ent, dataset.ent2idx\n",
        "    e_num = len(idx2ent)\n",
        "\n",
        "    r2mat = construct_rmat(idx2rel, idx2ent, ent2idx, fact_rdf + train_rdf + valid_rdf)\n",
        "\n",
        "    body2mat = {}\n",
        "\n",
        "    rule_dataset = RuleDataset(r2mat, rules, e_num, idx2rel, args)\n",
        "    rule_loader = DataLoader(\n",
        "        rule_dataset,\n",
        "        batch_size=args.data_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=max(1, args.cpu_num // 2),\n",
        "\n",
        "        collate_fn=RuleDataset.collate_fn,\n",
        "    )\n",
        "\n",
        "    for epoch, sample in enumerate(rule_loader):\n",
        "        heads, score_counts = sample\n",
        "        for idx in range(len(heads)):\n",
        "            head = heads[idx]\n",
        "            score_count = score_counts[idx]\n",
        "            body2mat[head] = score_count\n",
        "\n",
        "    for key, value in body2mat.items():\n",
        "        array_matrix = value.todense()\n",
        "        body2mat[key] = array_matrix\n",
        "\n",
        "    for q_i, query_rdf in enumerate(test_rdf):\n",
        "        query = parse_rdf(query_rdf)\n",
        "        q_h, q_r, q_t = query\n",
        "        if q_r not in body2mat:\n",
        "            continue\n",
        "        print(\"{}\\t{}\\t{}\".format(q_h, q_r, q_t))\n",
        "\n",
        "        pred = np.squeeze(np.array(body2mat[q_r][ent2idx[q_h]]))\n",
        "        pred_ranks = np.argsort(pred)[::-1]\n",
        "\n",
        "        truth = gt[(q_h, q_r)]\n",
        "        truth = [t for t in truth if t != ent2idx[q_t]]\n",
        "\n",
        "        L = []\n",
        "        H = []\n",
        "\n",
        "        top_rank = 0\n",
        "        mean_rank = 0\n",
        "\n",
        "        for i in range(len(pred_ranks)):\n",
        "            idx = pred_ranks[i]\n",
        "            if idx not in truth and pred[idx] > pred[ent2idx[q_t]]:\n",
        "                L.append(idx)\n",
        "            if idx not in truth and pred[idx] >= pred[ent2idx[q_t]]:\n",
        "                H.append(idx)\n",
        "\n",
        "        for i in range(len(L) + 1, len(H) + 1):\n",
        "            mean_rank += i / (len(H) - len(L))\n",
        "        Mean_rank['mrr'].append(1.0 / mean_rank)\n",
        "        Mean_rank['hits_1'].append(1 if mean_rank <= 1 else 0)\n",
        "        Mean_rank['hits_10'].append(1 if mean_rank <= 10 else 0)\n",
        "        head2Mean_rank_mrr[q_r].append(1.0 / mean_rank)\n",
        "        head2Mean_rank_hit_1[q_r].append(1 if mean_rank <= 1 else 0)\n",
        "        head2Mean_rank_hit_10[q_r].append(1 if mean_rank <= 10 else 0)\n",
        "        print(\"Number{}_use_mean_rank:{}\".format(q_i, mean_rank))\n",
        "\n",
        "        top_rank = len(L) + 1\n",
        "        Top_rank['mrr'].append(1.0 / top_rank)\n",
        "        Top_rank['hits_1'].append(1 if top_rank <= 1 else 0)\n",
        "        Top_rank['hits_10'].append(1 if top_rank <= 10 else 0)\n",
        "        head2Top_rank_mrr[q_r].append(1.0 / top_rank)\n",
        "        head2Top_rank_hit_1[q_r].append(1 if top_rank <= 1 else 0)\n",
        "        head2Top_rank_hit_10[q_r].append(1 if top_rank <= 10 else 0)\n",
        "        print(\"Number{}_use_top_rank:{}\".format(q_i, top_rank))\n",
        "\n",
        "    print(\"{:<16}{:<20} Hits@1:{:<20} Hits@10:{:<20}\\n{:>16}{:<20} Hits@1:{:<20} Hits@10:{:<20}\\n\".format(\n",
        "        \"expectation MRR:\", np.mean(Mean_rank['mrr']), np.mean(Mean_rank['hits_1']),\n",
        "        np.mean(Mean_rank['hits_10']), \"TOP MRR:\", np.mean(Top_rank['mrr']), np.mean(Top_rank['hits_1']),\n",
        "        np.mean(Top_rank['hits_10'])))\n",
        "\n",
        "    model_file = get_model_file(args)\n",
        "    os.makedirs(\"../evaluate/{}/{}\".format(args.model, args.datasets), exist_ok=True)\n",
        "    with open(\"../evaluate/{}/{}/{}_{}-{}_[{}].txt\".format(args.model, args.datasets, model_file, args.rule_len_low,\n",
        "                                                           args.rule_len_high, args.threshold), 'w') as f:\n",
        "        f.write(\"{:<16}{:<20} Hits@1:{:<20} Hits@10:{:<20}\\n{:>16}{:<20} Hits@1:{:<20} Hits@10:{:<20}\\n\".format(\n",
        "            \"expectation MRR:\", np.mean(Mean_rank['mrr']), np.mean(Mean_rank['hits_1']),\n",
        "            np.mean(Mean_rank['hits_10']), \"TOP MRR:\", np.mean(Top_rank['mrr']), np.mean(Top_rank['hits_1']),\n",
        "            np.mean(Top_rank['hits_10'])))\n",
        "\n",
        "        f.write('\\n{:<40}{:<20}{:<20}\\n'.format(\"head\", \"expectation MRR\", \"TOP MRR\"))\n",
        "        for (head, mrr1), (head, mrr2) in zip(head2Mean_rank_mrr.items(), head2Top_rank_mrr.items()):\n",
        "            f.write('{:<40}{:<20}{:<20}\\n'.format(head, np.mean(mrr1), np.mean(mrr2)))\n",
        "\n",
        "\n",
        "def feq(relation, fact_rdf):\n",
        "    count = 0\n",
        "    for rdf in fact_rdf:\n",
        "        h, r, t = parse_rdf(rdf)\n",
        "        if r == relation:\n",
        "            count += 1\n",
        "    return count\n"
      ],
      "metadata": {
        "id": "-BrC5NLf9Nb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from random import sample\n",
        "import random\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import time\n",
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "import debugpy\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(\"Current device:\", device) # Print the correct device being used\n",
        "\n",
        "\n",
        "\n",
        "rule_conf = {}\n",
        "candidate_rule = {}\n",
        "\n",
        "\n",
        "def sample_training_data(sample_max_path_len, anchor_num, fact_rdf, entity2desced, head_rdict):\n",
        "    print(\"Sampling training data...\")\n",
        "    anchors_rdf = []\n",
        "    per_anchor_num = anchor_num // ((head_rdict.__len__() - 1) // 2)\n",
        "    print(\"Number of head relation:{}\".format((head_rdict.__len__() - 1) // 2))\n",
        "    print(\"Number of per_anchor_num: {}\".format(per_anchor_num))\n",
        "\n",
        "    fact_dict = construct_fact_dict(fact_rdf)\n",
        "    for head in head_rdict.rel2idx:\n",
        "\n",
        "        if head != \"None\" and \"inv_\" not in head:\n",
        "            sampled_rdf = sample_anchor_rdf(fact_dict[head], per_anchor_num)\n",
        "            anchors_rdf.extend(sampled_rdf)\n",
        "\n",
        "    print(\"Total_anchor_num\", len(anchors_rdf))\n",
        "    train_rule, train_rule_dict = [], {}\n",
        "    len2train_rule_idx = {}\n",
        "\n",
        "    sample_number = 0\n",
        "\n",
        "    for anchor_rdf in anchors_rdf:\n",
        "        rule_seq, record = construct_rule_seq(fact_rdf, anchor_rdf, entity2desced, sample_max_path_len,\n",
        "                                              PRINT=False)\n",
        "\n",
        "        sample_number += len(record)\n",
        "        if len(rule_seq) > 0:\n",
        "            train_rule += rule_seq\n",
        "            for rule in rule_seq:\n",
        "                idx = torch.LongTensor(rule2idx(rule, head_rdict))\n",
        "                h = head_rdict.idx2rel[idx[-1].item()]\n",
        "\n",
        "                if h not in train_rule_dict:\n",
        "                    train_rule_dict[h] = []\n",
        "                train_rule_dict[h].append(idx)\n",
        "                body_len = len(idx) - 2\n",
        "                if body_len in len2train_rule_idx.keys():\n",
        "                    len2train_rule_idx[body_len] += [idx]\n",
        "                else:\n",
        "                    len2train_rule_idx[body_len] = [idx]\n",
        "\n",
        "    print(\"# head:{}\".format(len(train_rule_dict)))\n",
        "    for h in train_rule_dict:\n",
        "        print(\"head {}:{}\".format(h, len(train_rule_dict[h])))\n",
        "    rule_len_range = list(len2train_rule_idx.keys())\n",
        "    print(\"Fact set number:{} Sample number:{}\".format(len(fact_rdf), sample_number))\n",
        "    for rule_len in rule_len_range:\n",
        "        print(\"sampled examples for rule of length {}: {}\".format(rule_len, len(len2train_rule_idx[rule_len])))\n",
        "    print(\"length_of_train_rule:{}\".format(len(train_rule)))\n",
        "\n",
        "    return len2train_rule_idx\n",
        "\n",
        "\n",
        "def train(args, dataset):\n",
        "    rdict = dataset.get_relation_dict()\n",
        "    head_rdict = dataset.get_head_relation_dict()\n",
        "    all_rdf = dataset.fact_rdf + dataset.train_rdf + dataset.valid_rdf\n",
        "    entity2desced = construct_descendant(all_rdf)\n",
        "\n",
        "    relation_num = rdict.__len__()\n",
        "\n",
        "    sample_max_path_len = args.sample_max_path_len\n",
        "    anchor_num = args.anchor\n",
        "    len2train_rule_idx = sample_training_data(sample_max_path_len, anchor_num, all_rdf, entity2desced, head_rdict)\n",
        "    print_msg(\"  Start training  \")\n",
        "    batch_size = args.batch_size\n",
        "    emb_size = args.embedding_size\n",
        "    n_epoch = args.epochs\n",
        "    lr = args.learning_rate\n",
        "    body_len_range = list(range(args.learned_rule_len_from_x_to_X, args.learned_rule_len_from_2_to_X + 1))\n",
        "    print(\"body_len_range\", body_len_range)\n",
        "    model = Encoder(relation_num, emb_size, device)\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        if args.parallel:\n",
        "            device_ids = [0, 1]\n",
        "            model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "    loss_func_head = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "\n",
        "    train_acc = {}\n",
        "\n",
        "    for rule_len in body_len_range:\n",
        "        rule_ = len2train_rule_idx[rule_len]\n",
        "        print(\"\\nrule length:{}\".format(rule_len))\n",
        "\n",
        "        train_acc[rule_len] = []\n",
        "        for epoch in range(n_epoch):\n",
        "            model.zero_grad()\n",
        "\n",
        "            if len(rule_) > batch_size:\n",
        "                sample_rule_ = sample(rule_, batch_size)\n",
        "            else:\n",
        "                sample_rule_ = rule_\n",
        "            body_ = [r_[0:-2] for r_ in sample_rule_]\n",
        "            head_ = [r_[-1] for r_ in sample_rule_]\n",
        "            print(head_)\n",
        "            inputs_h = body_\n",
        "            targets_h = head_\n",
        "\n",
        "            inputs_h = torch.stack(inputs_h, 0).to(device)\n",
        "            targets_h = torch.stack(targets_h, 0).to(device)\n",
        "\n",
        "            pred_head, _entropy_loss, relation_emb_weigth = model(inputs_h)\n",
        "\n",
        "            loss_head = loss_func_head(pred_head, targets_h.reshape(-1))\n",
        "\n",
        "            entropy_loss = _entropy_loss.mean()\n",
        "            loss = args.alpha * loss_head + (1 - args.alpha) * entropy_loss\n",
        "\n",
        "            if epoch % (n_epoch // 10) == 0:\n",
        "                print(\"### epoch:{}\\tloss_head:{:.3}\\tentropy_loss:{:.3}\\tloss:{:.3}\\t\".format(epoch, loss_head,\n",
        "                                                                                               entropy_loss, loss))\n",
        "\n",
        "            train_acc[rule_len].append(\n",
        "                ((pred_head.argmax(dim=1) == targets_h.reshape(-1)).sum() / pred_head.shape[0]).cpu().numpy())\n",
        "            print(train_acc)\n",
        "            clip_grad_norm_(model.parameters(), 0.5)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.title(\"LogicFormer Epoch vs Accurary\")\n",
        "        train_acc[rule_len] = [float(x) for x in train_acc[rule_len]]\n",
        "        plt.plot(train_acc[rule_len])\n",
        "\n",
        "        os.makedirs(\"../figures/{}/{}/{}\".format(args.model, args.datasets, train_file_name), exist_ok=True)\n",
        "        plt.savefig('../figures/{}/{}/{}/{}.png'.format(args.model, args.datasets, train_file_name, rule_len))\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Time usage: {:.2}\".format(end - start))\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    os.makedirs(\"../results/{}/{}\".format(args.model, args.datasets, train_file_name), exist_ok=True)\n",
        "    with open('../results/{}/{}/{}'.format(args.model, args.datasets, train_file_name), 'wb') as g:\n",
        "        pickle.dump(model, g)\n",
        "\n",
        "\n",
        "def enumerate_body(relation_num, rdict, body_len):\n",
        "    import itertools\n",
        "    all_body_idx = list(list(x) for x in itertools.product(range(relation_num), repeat=body_len))\n",
        "\n",
        "    idx2rel = rdict.idx2rel\n",
        "    all_body = []\n",
        "    for b_idx_ in all_body_idx:\n",
        "        b_ = [idx2rel[x] for x in b_idx_]\n",
        "        all_body.append(b_)\n",
        "    return all_body_idx, all_body\n",
        "def train(args, dataset):\n",
        "    rdict = dataset.get_relation_dict()\n",
        "    head_rdict = dataset.get_head_relation_dict()\n",
        "    all_rdf = dataset.fact_rdf + dataset.train_rdf + dataset.valid_rdf\n",
        "    entity2desced = construct_descendant(all_rdf)\n",
        "\n",
        "    relation_num = rdict.__len__()\n",
        "\n",
        "    sample_max_path_len = args.sample_max_path_len\n",
        "    anchor_num = args.anchor\n",
        "    len2train_rule_idx = sample_training_data(sample_max_path_len, anchor_num, all_rdf, entity2desced, head_rdict)\n",
        "    print(\"len2train_rule_idx:\", len2train_rule_idx)  # Debugging line\n",
        "    print_msg(\"  Start training  \")\n",
        "    batch_size = args.batch_size\n",
        "    emb_size = args.embedding_size\n",
        "    n_epoch = args.epochs\n",
        "    lr = args.learning_rate\n",
        "    body_len_range = list(range(args.learned_rule_len_from_x_to_X, args.learned_rule_len_from_2_to_X + 1))\n",
        "    print(\"body_len_range:\", body_len_range)  # Debugging line\n",
        "    model = Encoder(relation_num, emb_size, device)\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        if args.parallel:\n",
        "            device_ids = [0, 1]\n",
        "            model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "    loss_func_head = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "\n",
        "    train_acc = {}\n",
        "\n",
        "    for rule_len in body_len_range:\n",
        "        if rule_len not in len2train_rule_idx:\n",
        "            print(f\"Warning: rule length {rule_len} not found in len2train_rule_idx\")  # Error handling line\n",
        "            continue\n",
        "        rule_ = len2train_rule_idx[rule_len]\n",
        "        print(\"\\nrule length:{}\".format(rule_len))\n",
        "\n",
        "        train_acc[rule_len] = []\n",
        "        for epoch in range(n_epoch):\n",
        "            model.zero_grad()\n",
        "\n",
        "            if len(rule_) > batch_size:\n",
        "                sample_rule_ = sample(rule_, batch_size)\n",
        "            else:\n",
        "                sample_rule_ = rule_\n",
        "            body_ = [r_[0:-2] for r_ in sample_rule_]\n",
        "            head_ = [r_[-1] for r_ in sample_rule_]\n",
        "            print(\"head\",head_)\n",
        "            inputs_h = body_\n",
        "            targets_h = head_\n",
        "\n",
        "            inputs_h = torch.stack(inputs_h, 0).to(device)\n",
        "            targets_h = torch.stack(targets_h, 0).to(device)\n",
        "\n",
        "            pred_head, _entropy_loss, relation_emb_weigth = model(inputs_h)\n",
        "\n",
        "            loss_head = loss_func_head(pred_head, targets_h.reshape(-1))\n",
        "            print(targets_h)\n",
        "            entropy_loss = _entropy_loss.mean()\n",
        "            loss = args.alpha * loss_head + (1 - args.alpha) * entropy_loss\n",
        "\n",
        "            if epoch % (n_epoch // 10) == 0:\n",
        "                print(\"### epoch:{}\\tloss_head:{:.3}\\tentropy_loss:{:.3}\\tloss:{:.3}\\t\".format(epoch, loss_head,\n",
        "                                                                                               entropy_loss, loss))\n",
        "\n",
        "            train_acc[rule_len].append(\n",
        "                ((pred_head.argmax(dim=1) == targets_h.reshape(-1)).sum() / pred_head.shape[0]).cpu().numpy())\n",
        "\n",
        "            clip_grad_norm_(model.parameters(), 0.5)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.title(\"LogicFormer Epoch vs Accurary\")\n",
        "        train_acc[rule_len] = [float(x) for x in train_acc[rule_len]]\n",
        "        plt.plot(train_acc[rule_len])\n",
        "\n",
        "        os.makedirs(\"../figures/{}/{}/{}\".format(args.model, args.datasets, train_file_name), exist_ok=True)\n",
        "        plt.savefig('../figures/{}/{}/{}/{}.png'.format(args.model, args.datasets, train_file_name, rule_len))\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Time usage: {:.2}\".format(end - start))\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    os.makedirs(\"../results/{}/{}\".format(args.model, args.datasets, train_file_name), exist_ok=True)\n",
        "    with open('../results/{}/{}/{}'.format(args.model, args.datasets, train_file_name), 'wb') as g:\n",
        "        pickle.dump(model, g)\n",
        "\n",
        "\n",
        "def test(args, dataset):\n",
        "    head_rdict = dataset.get_head_relation_dict()\n",
        "    print(head_rdict)\n",
        "    with open('../results/{}/{}/{}'.format(args.model, args.datasets, train_file_name),\n",
        "              'rb') as g:\n",
        "        if torch.cuda.is_available():\n",
        "            model = pickle.load(g)\n",
        "            model.to(device)\n",
        "            print_msg(str(device))\n",
        "        else:\n",
        "            model = torch.load(g, map_location='cpu')\n",
        "    print_msg(\"  Start Eval  \")\n",
        "    model.eval()\n",
        "\n",
        "    r_num = head_rdict.__len__() - 1\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    for i in range(args.learned_rule_len_from_x_to_X, args.learned_rule_len_from_2_to_X + 1):\n",
        "        rule_len = i\n",
        "        print(\"\\nrule length:{}\".format(rule_len))\n",
        "\n",
        "        probs = []\n",
        "        _, body = enumerate_body(r_num, head_rdict,\n",
        "                                 body_len=rule_len)\n",
        "        body_list = [\"|\".join(b) for b in body]\n",
        "        candidate_rule[rule_len] = body_list\n",
        "        n_epoches = math.ceil(float(len(body_list)) / batch_size)\n",
        "        for epoches in range(n_epoches):\n",
        "            bodies = body_list[epoches: (epoches + 1) * batch_size]\n",
        "            if epoches == n_epoches - 1:\n",
        "                bodies = body_list[epoches * batch_size:]\n",
        "            else:\n",
        "                bodies = body_list[epoches * batch_size: (epoches + 1) * batch_size]\n",
        "\n",
        "            body_idx = body2idx(bodies, head_rdict)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = torch.LongTensor(np.array(body_idx)).to(device)\n",
        "            else:\n",
        "                inputs = torch.LongTensor(np.array(body_idx))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_head, _entropy_loss, relation_emb_weigth = model(inputs)\n",
        "\n",
        "                prob_ = torch.softmax(pred_head, dim=-1)\n",
        "                probs.append(prob_.detach().cpu())\n",
        "\n",
        "        rule_conf[rule_len] = torch.cat(probs, dim=0)\n",
        "        #print(\"rule_conf[{}].shape:{}\".format(rule_len, rule_conf[rule_len].shape))\n",
        "        if args.get_rule:\n",
        "            print_msg(\"Generate Rule!\")\n",
        "            head_rdict = dataset.get_head_relation_dict()\n",
        "            n_rel = head_rdict.__len__() - 1\n",
        "            os.makedirs(\"../rules/{}/{}/{}\".format(args.model, args.datasets, model_file), exist_ok=True)\n",
        "            rule_path = '../rules/{}/{}/{}/{}.txt'.format(args.model, args.datasets, model_file, rule_len)\n",
        "            #print(\"\\nGenerate rule length:{}\".format(rule_len))\n",
        "            sorted_val, sorted_idx = torch.sort(rule_conf[rule_len], 0, descending=True)\n",
        "            n_rules, _ = sorted_val.shape\n",
        "            with open(rule_path, 'w') as g:\n",
        "                for r in range(n_rel):\n",
        "\n",
        "                    head = head_rdict.idx2rel[r]\n",
        "                    idx = 0\n",
        "                    while idx < args.get_top_k and idx < n_rules:\n",
        "                        conf = sorted_val[idx, r]\n",
        "                        body = candidate_rule[rule_len][sorted_idx[idx, r]]\n",
        "                        msg = \"{:.3f} ({:.3f})\\t{} <-- \".format(conf, conf, head)\n",
        "                        body = body.split('|')\n",
        "                        msg += \", \".join(body)\n",
        "                        g.write(msg + '\\n')\n",
        "                        idx += 1\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "def get_true_labels_for_bodies(bodies, dataset):\n",
        "    true_labels = []\n",
        "    for body in bodies:\n",
        "        label = dataset.get_label(body)  # Use the method you defined in the Dataset class\n",
        "        true_labels.append(label)\n",
        "    return true_labels\n",
        "\n",
        "def test(args, dataset):\n",
        "    head_rdict = dataset.get_head_relation_dict()\n",
        "    print(head_rdict)\n",
        "    with open('../results/{}/{}/{}'.format(args.model, args.datasets, train_file_name), 'rb') as g:\n",
        "        if torch.cuda.is_available():\n",
        "            model = pickle.load(g)\n",
        "            model.to(device)\n",
        "            print_msg(str(device))\n",
        "        else:\n",
        "            model = torch.load(g, map_location='cpu')\n",
        "    print_msg(\"  Start Eval  \")\n",
        "    model.eval()\n",
        "\n",
        "    r_num = head_rdict.__len__() - 1\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for i in range(args.learned_rule_len_from_x_to_X, args.learned_rule_len_from_2_to_X + 1):\n",
        "        rule_len = i\n",
        "        print(\"\\nrule length:{}\".format(rule_len))\n",
        "\n",
        "        probs = []\n",
        "        _, body = enumerate_body(r_num, head_rdict, body_len=rule_len)\n",
        "        body_list = [\"|\".join(b) for b in body]\n",
        "        candidate_rule[rule_len] = body_list\n",
        "        n_epoches = math.ceil(float(len(body_list)) / batch_size)\n",
        "        for epoches in range(n_epoches):\n",
        "            if epoches == n_epoches - 1:\n",
        "                bodies = body_list[epoches * batch_size:]\n",
        "            else:\n",
        "                bodies = body_list[epoches * batch_size: (epoches + 1) * batch_size]\n",
        "\n",
        "            body_idx = body2idx(bodies, head_rdict)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = torch.LongTensor(np.array(body_idx)).to(device)\n",
        "            else:\n",
        "                inputs = torch.LongTensor(np.array(body_idx))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_head, _entropy_loss, relation_emb_weigth = model(inputs)\n",
        "\n",
        "                prob_ = torch.softmax(pred_head, dim=-1)\n",
        "                predictions = prob_.argmax(dim=-1).cpu().numpy()\n",
        "                all_predictions.extend(predictions)\n",
        "\n",
        "                # Assuming you have a way to get true labels for each body\n",
        "                true_labels = get_true_labels_for_bodies(bodies, dataset)  # You need to implement this\n",
        "                all_labels.extend(true_labels)\n",
        "\n",
        "                probs.append(prob_.detach().cpu())\n",
        "\n",
        "        rule_conf[rule_len] = torch.cat(probs, dim=0)\n",
        "\n",
        "    # Calculating metrics\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision = precision_score(all_labels, all_predictions, average='macro')\n",
        "    recall = recall_score(all_labels, all_predictions, average='macro')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "    print(\"Accuracy: {:.3f}, Precision: {:.3f}, Recall: {:.3f}, F1 Score: {:.3f}\".format(accuracy, precision, recall, f1))\n",
        "\n",
        "\n",
        "def get_model_file(args):\n",
        "    model_file = '[epochs{}][alpha{}][anchor{}][max_sample{}][emb{}][lr{}][batchsize{}][topk{}]'.format(\n",
        "        args.epochs,\n",
        "        args.alpha,\n",
        "        args.anchor,\n",
        "        args.sample_max_path_len,\n",
        "        args.embedding_size,\n",
        "        args.learning_rate,\n",
        "        args.batch_size,\n",
        "        args.get_top_k\n",
        "    )\n",
        "    return model_file\n",
        "\n",
        "\n",
        "def get_train_file_name(args):\n",
        "    train_file_name = '[epochs{}][alpha{}][anchor{}][max_sample{}][emb{}][lr{}][batchsize{}]'.format(\n",
        "        args.epochs,\n",
        "        args.alpha,\n",
        "        args.anchor,\n",
        "        args.sample_max_path_len,\n",
        "        args.embedding_size,\n",
        "        args.learning_rate,\n",
        "        args.batch_size\n",
        "    )\n",
        "    return train_file_name\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    msg = \"First Order Logic Rule Mining\"\n",
        "    print(msg)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f', '--file', help='Dummy argument to avoid error')\n",
        "    parser.add_argument(\"--get_rule\", default=\"1\", action=\"store_true\", help=\"increase output verbosity\")\n",
        "    parser.add_argument(\"--gpu\", type=int, default=0, help=\"increase output verbosity\")\n",
        "    parser.add_argument(\"--parallel\", default=\"\", help=\"increase output verbosity\")\n",
        "    parser.add_argument(\"--sparsity\", type=float, default=1, help=\"increase output verbosity\")\n",
        "    parser.add_argument(\"--model\", default=\"show\", help=\"训练谁的模型\")\n",
        "    parser.add_argument(\"--datasets\", default=\"tic\", help=\"数据集\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1000, help=\"epochs\")\n",
        "    parser.add_argument(\"--alpha\", type=float, default=0.8, help=\"increase output verbosity\")\n",
        "    parser.add_argument(\"--beta\", type=float, default=3, help=\"control beta\")\n",
        "    parser.add_argument(\"--gamma\", type=float, default=3, help=\"control gamma\")\n",
        "    #parser.add_argument(\"--temp\", type=float, default=0.001, help=\"control temperature\")\n",
        "    parser.add_argument(\"--anchor\", type=int, default=5000, help=\"increase output verbosity\")\n",
        "    parser.add_argument(\"--sample_max_path_len\", type=int, default=2, help=\"采样路径长度\")\n",
        "    parser.add_argument(\"--embedding_size\", type=int, default=512, help=\"embedding_size\")\n",
        "    parser.add_argument(\"--learning_rate\", type=int, default=0.001, help=\"learning_rate\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=1000, help=\"increase output verbosity\")\n",
        "    parser.add_argument(\"--get_top_k\", type=int, default=100, help=\"得到规则的get_top_k\")\n",
        "    parser.add_argument(\"--learned_rule_len_from_x_to_X\", type=int, default=2, help=\"学习规则的长度\")\n",
        "    parser.add_argument(\"--learned_rule_len_from_2_to_X\", type=int, default=2, help=\"学习规则的长度\")\n",
        "    parser.add_argument('--cpu_num', type=int, default=20)\n",
        "    parser.add_argument(\"--data_batch_size\", type=int, default=1)\n",
        "    parser.add_argument(\"--threshold\", type=float, default=0)\n",
        "    parser.add_argument(\"--rule_len_low\", type=int, default=2)\n",
        "    parser.add_argument(\"--rule_len_high\", type=int, default=2)\n",
        "    args = parser.parse_args()\n",
        "    model_file = get_model_file(args)\n",
        "    train_file_name = get_train_file_name(args)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "    data_path = '/content/CARL/datasets/{}/'.format(args.datasets)\n",
        "    dataset = Dataset(data_root=data_path, sparsity=args.sparsity,\n",
        "                      inv=True)\n",
        "    print(\"Dataset:{}\".format(data_path))\n",
        "    model_path = \"../results/{}/{}/{}\".format(args.model, args.datasets, train_file_name)\n",
        "    print(\"results at:{}\".format(model_path))\n",
        "    if not os.path.isfile(\"../results/{}/{}/{}\".format(args.model, args.datasets, train_file_name)):\n",
        "        print(\"Train!\")\n",
        "        train(args, dataset)\n",
        "    print_msg(\"Test!\")\n",
        "    test(args, dataset)\n",
        "    all_rules = {}\n",
        "    all_rule_heads = []\n",
        "    for L in range(args.rule_len_low, args.rule_len_high + 1):\n",
        "        file = \"../rules/{}/{}/{}/{}.txt\".format(args.model, args.datasets, model_file, L)\n",
        "        load_rules(\"{}\".format(file), all_rules, all_rule_heads)\n",
        "    for head in all_rules:\n",
        "        all_rules[head] = all_rules[head][:args.get_top_k * 5]\n",
        "    fact_rdf, train_rdf, valid_rdf, test_rdf = dataset.fact_rdf, dataset.train_rdf, dataset.valid_rdf, dataset.test_rdf\n",
        "    kg_completion(all_rules, dataset, args)"
      ],
      "metadata": {
        "id": "gfd-vgxZ9Qgm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}