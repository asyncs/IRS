{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#RULE PREPROCESSING"
      ],
      "metadata": {
        "id": "qZSnmjBU6rfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the file\n",
        "file_path = '/content/rrl.txt'\n",
        "output_file_path = '/content/corrected_converted_rrl.txt'\n",
        "df = pd.read_csv(file_path, delimiter='\\t', skiprows=1, engine='python', header=None)\n",
        "\n",
        "\n",
        "df.columns = ['RID', 'class_negative', 'class_positive', 'Support', 'Rule']\n",
        "df['Rule'] = df['Rule'].astype(str)\n",
        "def format_rule(row):\n",
        "    conditions = re.sub(r'\\b\\d+_', '', row['Rule'])\n",
        "    if row['class_negative'] > row['class_positive']:\n",
        "        return f\"negative <-- {conditions}\"\n",
        "    else:\n",
        "        return f\"positive <-- {conditions}\"\n",
        "\n",
        "df['Formatted Rule'] = df.apply(format_rule, axis=1)\n",
        "\n",
        "with open(output_file_path, 'w') as f:\n",
        "    for rule in df['Formatted Rule']:\n",
        "        f.write(rule + '\\n')\n",
        "\n",
        "print(f'Formatted rules have been saved to {output_file_path}')\n"
      ],
      "metadata": {
        "id": "lBU8L_oL6uRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Path to the input file\n",
        "input_path = '/content/CARL.txt'\n",
        "output_path = '/content/Corrected_High_Confidence_Rules.txt'\n",
        "\n",
        "# Read the file\n",
        "with open(input_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "processed_lines = [re.sub(r'\\d+\\.\\d+ \\(\\d+\\.\\d+\\)\\s+', '', line) for line in lines]\n",
        "with open(output_path, 'w') as file:\n",
        "    file.writelines(processed_lines)\n",
        "\n",
        "print(f'Processed file saved to {output_path}')\n"
      ],
      "metadata": {
        "id": "6ItWkY-i7ndq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "Io7v6uCq6m8x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmLZ7PjB5uf-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "def read_and_correctly_transform_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    transformed_lines = []\n",
        "    for line in lines:\n",
        "        tokens = re.split(r', (?=\\w+ (?:on|at) )', line.strip())\n",
        "\n",
        "        processed_tokens = []\n",
        "        for token in tokens:\n",
        "            parts = token.split()\n",
        "            if parts[0] == \"inv\":\n",
        "                # Handle \"inv_\" cases with \"on\" or \"at\"\n",
        "                if parts[2] in [\"on\", \"at\"]:\n",
        "                    processed_tokens.append(f\"inv_{parts[1]} {parts[2]} {parts[3]}\")\n",
        "                else:\n",
        "                    processed_tokens.append(\"inv_\" + token)\n",
        "            else:\n",
        "                processed_tokens.append(token)\n",
        "\n",
        "        transformed_line = ', '.join(processed_tokens)\n",
        "        transformed_lines.append(transformed_line)\n",
        "\n",
        "    return transformed_lines\n",
        "\n",
        "input_file_path = '/content/transformed_data_no_index.data'\n",
        "correct_transformed_data = read_and_correctly_transform_data(input_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def create_json_from_all_data_corrected(data_lines):\n",
        "    json_data = []\n",
        "    for line in data_lines:\n",
        "        features = line.split(',')\n",
        "        data_dict = {feature.strip(): 0 if feature.strip().startswith('inv_') else 1 for feature in features if not 'negative' in feature.strip()}\n",
        "        label_key = [f for f in features if 'negative' in f.strip()]\n",
        "        if label_key:\n",
        "            data_dict['label'] = 'negative'\n",
        "        json_data.append(data_dict)\n",
        "\n",
        "    return json_data\n",
        "all_data_json_corrected = create_json_from_all_data_corrected(correct_transformed_data)\n",
        "\n",
        "json_file_path_corrected = '/content/transformed_data_json_corrected.json'\n",
        "with open(json_file_path_corrected, 'w') as json_file:\n",
        "    json.dump(all_data_json_corrected, json_file, indent=4)\n",
        "\n",
        "print(\"JSON file saved at:\", json_file_path_corrected)\n"
      ],
      "metadata": {
        "id": "k1XBiTGW6PN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path_corrected = '/content/transformed_data_json_corrected.json'\n",
        "with open(json_file_path_corrected, 'r') as json_file:\n",
        "    all_data_json_corrected = json.load(json_file)\n",
        "\n",
        "def process_data(data):\n",
        "\n",
        "    label_key = None\n",
        "    label_value = None\n",
        "    for key, value in list(data.items()):\n",
        "        if key.endswith(\"negative\") or key.endswith(\"positive\"):\n",
        "            label_key = 'label'\n",
        "            label_value = key.split('_')[-1]\n",
        "            del data[key]\n",
        "    new_data = data.copy()\n",
        "    for key, value in data.items():\n",
        "        if key.startswith(\"inv_\") and value == 1:\n",
        "            new_key = key[4:]\n",
        "            if new_key not in data:\n",
        "                new_data[new_key] = 0\n",
        "        elif not key.startswith(\"inv_\") and value == 1:\n",
        "            new_key = \"inv_\" + key\n",
        "            if new_key not in data:\n",
        "                new_data[new_key] = 0\n",
        "    if label_key and label_value:\n",
        "        new_data[label_key] = label_value\n",
        "\n",
        "    return new_data\n",
        "\n",
        "processed_data = [process_data(data) for data in all_data_json_corrected]\n",
        "processed_json_file_path = '/content/processed_transformed_data_json.json'\n",
        "with open(processed_json_file_path, 'w') as json_file:\n",
        "    json.dump(processed_data, json_file, indent=4)\n",
        "\n",
        "processed_json_file_path\n"
      ],
      "metadata": {
        "id": "bLRRD_-E6eSb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}